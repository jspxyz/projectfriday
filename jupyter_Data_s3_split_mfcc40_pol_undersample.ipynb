{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
    "from tensorflow.keras.layers import Activation, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D, Conv2D, LSTM\n",
    "from tensorflow.keras.layers import AveragePooling1D, GlobalAveragePooling2D, MaxPooling1D\n",
    "from tensorflow.keras.models import Model, model_from_json, Sequential\n",
    "\n",
    "# added this to plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error list:  0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Data Processing Part 3\n",
    "Splitting with MFCC 40 features\n",
    "Labels: Polarity\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# calling the Data Path file\n",
    "ref_data_path = pd.read_csv('./Data_Array_Storage/Data_path.csv')\n",
    "\n",
    "# opening df_features\n",
    "with open('./Data_Array_Storage/data_features_mfcc40.pkl', 'rb') as f:\n",
    "    df_features = pickle.load(f)\n",
    "\n",
    "# opening df_features_noise as pickle file\n",
    "with open('./Data_Array_Storage/data_features_noise_mfcc40.pkl', 'rb') as f:\n",
    "    df_features_noise = pickle.load(f)\n",
    "\n",
    "# opening error_list as pickle file\n",
    "with open('./Data_Array_Storage/error_list_mfcc40.pkl', 'rb') as f:\n",
    "    error_list = pickle.load(f)\n",
    "\n",
    "print(\"Error list: \", len(error_list))\n",
    "\n",
    "# changing lists into numpy arrays\n",
    "# ref_data_path_array = np.array(ref_data_path) # do not need this\n",
    "df_features_array = np.array(df_features)\n",
    "df_features_noise_array = np.array(df_features_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_y_full:  (24324, 7)\n",
      "X_full:  (24324, 216, 40)\n",
      "y_full after drop:  ['negative' 'negative' 'neutral' 'positive' 'neutral']\n"
     ]
    }
   ],
   "source": [
    "# creating a y table that matches X table by doubling the ref_data_path\n",
    "df_y_full = pd.concat((ref_data_path, ref_data_path),axis=0)\n",
    "print('df_y_full: ', df_y_full.shape)\n",
    "\n",
    "# creating X table of all dataset\n",
    "X_full = np.concatenate((df_features, df_features_noise),axis=0)\n",
    "print('X_full: ', X_full.shape)\n",
    "\n",
    "# drop the columns\n",
    "# 'gender','emotion','label_emotion', 'polarity', 'label_polarity','path','source'\n",
    "# keep polarity\n",
    "y_full = df_y_full.drop(['gender','emotion','label_emotion','label_polarity','path','source'],axis=1).to_numpy().squeeze()\n",
    "\n",
    "print('y_full after drop: ', y_full[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_full = X_full.reshape((24324,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24324, 8640)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24324,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'negative', 'neutral', ..., 'positive', 'neutral',\n",
       "       'negative'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_X = new_X_full[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_Y = y_full[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'negative': 15384, 'positive': 5150, 'neutral': 3790})\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(y_full)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'negative': 17, 'neutral': 17, 'positive': 17})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "undersample = NearMiss (sampling_strategy = \"not minority\")\n",
    "\n",
    "X_under, y_under = undersample.fit_resample(small_X, small_Y)\n",
    "from collections import Counter\n",
    "counter = Counter(y_under)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 8640)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_under.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_under,\n",
    "                                                     y_under,\n",
    "                                                     test_size=0.1,\n",
    "                                                     shuffle=True,\n",
    "                                                     random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 8640)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = X_train.reshape((45,216,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 216, 40)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 8640)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_final = X_test.reshape((6,216,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 216, 40)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'positive', 'negative', 'negative', 'negative'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'neutral', 'positive', 'positive', 'negative'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the target \n",
    "lb = LabelEncoder()\n",
    "y_train_hot = tf.keras.utils.to_categorical(lb.fit_transform(y_train)) # tf.keras.utils.to_categorical\n",
    "y_test_hot = tf.keras.utils.to_categorical(lb.transform(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_hot[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_hot[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = X_train.reshape((X_train.shape[0], X_full.shape[1], X_full.shape[2]))\n",
    "X_test_final = X_test.reshape((X_test.shape[0], X_full.shape[1], X_full.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 216, 40)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 216, 40)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_d_conv1d(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, 3, padding='same',input_shape=input_shape))  # X_train.shape[1] = No. of Columns (216)\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(32, 3, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(MaxPooling1D(pool_size=(3)))\n",
    "    model.add(Conv1D(64, 3, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(64, 3, padding='same'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(MaxPooling1D(pool_size=(3))) # added drop out and maxpooling layer on 20201209 at 1330\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(64, 3, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(64, 3, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(MaxPooling1D(pool_size=(3)))\n",
    "    model.add(Conv1D(128, 3, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(128, 3, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv1D(256, 3, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "#     model.add(LSTM(64))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(3)) # Target class number\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # model optimizer\n",
    "#     model = model_d_conv1d(input_shape)\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (X_train_final.shape[1], X_train_final.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape, model, optimizer loaded\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 216, 32)           3872      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 216, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 216, 32)           3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 216, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 216, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 216, 32)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 72, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 72, 64)            6208      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 72, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 72, 64)            12352     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 72, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 24, 64)            0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 24, 64)            12352     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 24, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 24, 64)            12352     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 24, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 24, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 8, 128)            24704     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 8, 128)            49280     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 8, 256)            98560     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 8, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 6147      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 229,315\n",
      "Trainable params: 229,123\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_d_conv1d(input_shape)\n",
    "# optimizer = tf.keras.optimizers.RMSprop(lr=0.000001, decay=1e-6)\n",
    "\n",
    "print('input shape, model, optimizer loaded')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "callbacks and checkpoints set\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./models_saved/model_d_conv1d_mfcc40_undersample100_pol.h5\"\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1), # 1 tells your which epoch is saving\n",
    "#                                                  monitor='val_categorical_accuracy',  # added for emo1d\n",
    "#                                                  mode='max'), # added for emo1d\n",
    "               tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', # changed from 'val_accuracy', 'val_loss'\n",
    "                                                patience=5, \n",
    "                                                restore_best_weights=True),\n",
    "#                                                 verbose = 1, # added verbose for emo1d\n",
    "#                                                 mode = 'min'), # added for emo1d\n",
    "               tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "                                                    patience=2, \n",
    "                                                    factor=0.5, \n",
    "                                                    min_lr=0.000001, \n",
    "                                                    verbose=1)]\n",
    "\n",
    "print('callbacks and checkpoints set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.29924, saving model to ./models_saved/model_d_conv1d_mfcc40_undersample100_pol.h5\n",
      "3/3 - 0s - loss: 1.1521 - accuracy: 0.3333 - val_loss: 1.2992 - val_accuracy: 0.3333\n",
      "Epoch 2/150\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.29924 to 1.20781, saving model to ./models_saved/model_d_conv1d_mfcc40_undersample100_pol.h5\n",
      "3/3 - 0s - loss: 0.9590 - accuracy: 0.5333 - val_loss: 1.2078 - val_accuracy: 0.3333\n",
      "Epoch 3/150\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.20781\n",
      "3/3 - 0s - loss: 0.5834 - accuracy: 0.7333 - val_loss: 1.6271 - val_accuracy: 0.3333\n",
      "Epoch 4/150\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.20781\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "3/3 - 0s - loss: 0.3987 - accuracy: 0.9111 - val_loss: 2.6583 - val_accuracy: 0.3333\n",
      "Epoch 5/150\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.20781\n",
      "3/3 - 0s - loss: 0.3547 - accuracy: 0.8667 - val_loss: 3.3734 - val_accuracy: 0.3333\n",
      "Epoch 6/150\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.20781\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "3/3 - 0s - loss: 0.1830 - accuracy: 0.9333 - val_loss: 4.7261 - val_accuracy: 0.3333\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "model_history=model.fit(X_train_final, \n",
    "                        y_train_hot,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=150,\n",
    "                        validation_data=(X_test_final, y_test_hot),\n",
    "                        verbose=2,\n",
    "                        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# set indices to randomize\n",
    "indices = np.random.permutation(len(X_full))\n",
    "train_size = 0.8\n",
    "len_train_set = int(len(X_full) * train_size)\n",
    "\n",
    "X_shuffle = X_full[indices]\n",
    "y_shuffle = y_full[indices]\n",
    "X_train = X_shuffle[:len_train_set]\n",
    "y_train = y_shuffle[:len_train_set]\n",
    "\n",
    "X_test = X_shuffle[len_train_set:]\n",
    "y_test = y_shuffle[len_train_set:]\n",
    "\n",
    "print('shapes after np.random.permutation splits')\n",
    "print('X_train: ', X_train.shape)\n",
    "print('y_train: ', y_train.shape)\n",
    "print('X_test: ', X_test.shape)\n",
    "print('y_test: ', y_test.shape)\n",
    "\n",
    "print('y_test header 5')\n",
    "print(y_test[:5])\n",
    "\n",
    "print('X_test header 3')\n",
    "print(X_test[:3])\n",
    "\n",
    "# combining path with features\n",
    "# changing features to list\n",
    "# df_path_features = pd.concat([ref_data_path,pd.DataFrame(df_features['feature'].values.tolist())],axis=1)\n",
    "# df_path_features_noise = pd.concat([ref_data_path,pd.DataFrame(df_features_noise['feature'].values.tolist())],axis=1)\n",
    "\n",
    "# df_features_all = pd.concat([df_path_features,df_path_features_noise],axis=0,sort=False) # ,df_speedpitch\n",
    "# df_final = df_features_all.fillna(0)\n",
    "\n",
    "# df_final = pd.concat([ref_data_path, pd.DataFrame(df_features)],axis=1)\n",
    "\n",
    "# # Split between train and test \n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_final.drop(['gender','emotion','label_emotion','polarity','label_polarity','path','source'],axis=1),\n",
    "#                                                     df_final.polarity,\n",
    "#                                                     test_size=0.25,\n",
    "#                                                     shuffle=True,\n",
    "#                                                     random_state=42)\n",
    "\n",
    "# print('Shape after train_test_split')\n",
    "# print('X_train: ', X_train.shape)\n",
    "# print('X_test: ', X_test.shape)\n",
    "# print('y_train: ', y_train.shape)\n",
    "# print('y_test: ', y_test.shape)\n",
    "\n",
    "# Data normalization \n",
    "# original \n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "# new methood to data normazilie over each individual\n",
    "# mean = np.mean(np.reshape(X_train, (X_train.shape[0], -1)), axis=1) # (1000,)\n",
    "# std = np.std(np.reshape(X_train, (X_train.shape[0], -1)), axis=1)   # (1000,)\n",
    "\n",
    "X_train = (X_train - mean)/std\n",
    "X_test = (X_test - mean)/std\n",
    "\n",
    "# Preparation steps to get it into the correct format for Keras \n",
    "# X_train = np.array(X_train)\n",
    "# y_train = np.array(y_train)\n",
    "# X_test = np.array(X_test)\n",
    "# y_test = np.array(y_test)\n",
    "\n",
    "print('Shape after data normalization for X_ only')\n",
    "print('X_train: ', X_train.shape)\n",
    "print('X_test: ', X_test.shape)\n",
    "print('y_train: ', y_train.shape)\n",
    "print('y_test: ', y_test.shape)\n",
    "\n",
    "# one hot encode the target \n",
    "lb = LabelEncoder()\n",
    "y_train = tf.keras.utils.to_categorical(lb.fit_transform(y_train)) # tf.keras.utils.to_categorical\n",
    "y_test = tf.keras.utils.to_categorical(lb.fit_transform(y_test))\n",
    "\n",
    "print('Shape after one hot encode (for y_ only)')\n",
    "print('X_train: ', X_train.shape)\n",
    "print('X_test: ', X_test.shape)\n",
    "print('y_train: ', y_train.shape)\n",
    "print('y_test: ', y_test.shape)\n",
    "\n",
    "print('y_test top 5', y_test[:5])\n",
    "\n",
    "# save y_train, y_test\n",
    "with open('./Data_Array_Storage/y_train_mfcc40_axis0_pol.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "\n",
    "with open('./Data_Array_Storage/y_test_mfcc40_axis0_pol.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "\n",
    "# Pickel the lb object for future use \n",
    "with open('./Data_Array_Storage/labels_mfcc40_axis0_pol.pkl', 'wb') as f:\n",
    "    pickle.dump(lb, f)\n",
    "\n",
    "# expanding X_train and X_test dimensions\n",
    "# no need to do this for conv1d\n",
    "# X_train = np.expand_dims(X_train, axis=-1)\n",
    "# X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "# print('Shape after X dimension expansion')\n",
    "# print('X_train: ', X_train.shape)\n",
    "# print('X_test: ', X_test.shape)\n",
    "# print('y_train: ', y_train.shape)\n",
    "# print('y_test: ', y_test.shape)\n",
    "\n",
    "# saving X_train and X_test\n",
    "with open('./Data_Array_Storage/X_train_mfcc40_axis0_pol.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train, f)\n",
    "\n",
    "with open('./Data_Array_Storage/X_test_mfcc40_axis0_pol.pkl', 'wb') as f:\n",
    "    pickle.dump(X_test, f)\n",
    "\n",
    "print('Pickle files saved. Final shpaes:')\n",
    "print('X_train: ', X_train.shape)\n",
    "print('X_test: ', X_test.shape)\n",
    "print('y_train: ', y_train.shape)\n",
    "print('y_test: ', y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
